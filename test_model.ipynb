{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch, torchvision\n",
        "from pix2code import Vocabulary, Pix2codeDataset, EncoderCNN, DecoderRNN, collate_fn"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## model parameters saved"
      ],
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 2\n",
        "num_workers = 1\n",
        "embed_size = 256\n",
        "hidden_size = 512\n",
        "num_layers = 1\n",
        "crop_size = 224\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "outputs": [],
      "execution_count": 24,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %% Paths\n",
        "data_path = \"../data/data_bootstrap/\"\n",
        "#train_path = data_path + \"processed_data/data_train/\"\n",
        "test_path = data_path + \"processed_data/data_test/\"\n",
        "vocab_path = data_path + \"bootstrap.vocab\""
      ],
      "outputs": [],
      "execution_count": 25,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(vocab_path, 'r') as file:\n",
        "    content = file.read()\n",
        "words = content.split(' '); del content\n",
        "len(words)\n",
        "\n",
        "vocab = Vocabulary()\n",
        "for word in words:\n",
        "    vocab.add_word(word)\n",
        "vocab.add_word(' ')\n",
        "vocab.add_word('<unk>') # if we find an unknown word\n",
        "vocab_size = len(vocab)\n",
        "print(f\"total length of vocab is : {vocab_size}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total length of vocab is : 19\n"
          ]
        }
      ],
      "execution_count": 26,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.Resize((crop_size, crop_size)), # Match resnet size\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    # See for : http://pytorch.org/docs/master/torchvision/models.html\n",
        "    torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "test_dataset = Pix2codeDataset(data_path=test_path, vocab=vocab, transform=transform)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size,\n",
        "                         shuffle=True, num_workers=num_workers, collate_fn=collate_fn)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created dataset of   175 items from\n",
            "../data/data_bootstrap/processed_data/data_test/\n"
          ]
        }
      ],
      "execution_count": 27,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = EncoderCNN(embed_size).to(device)\n",
        "decoder = DecoderRNN(embed_size, hidden_size, len(vocab), num_layers).to(device)\n",
        "\n",
        "folder_ = \"./Logs/2018-08-04-03-40-07/\"\n",
        "epoch_ = 100\n",
        "encoder.load_state_dict(torch.load(f\"{folder_}encoder_epoch{epoch_}.pkl\",map_location='cpu'))\n",
        "decoder.load_state_dict(torch.load(f\"{folder_}decoder_epoch{epoch_}.pkl\",map_location='cpu'))"
      ],
      "outputs": [],
      "execution_count": 28,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for index in target[1:-1].numpy().astype(int):\n",
        "    print(vocab.idx2word[index], end=\"\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "header{btn-active,btn-inactive,btn-inactive,btn-inactive}row{quadruple{small-title,text,btn-orange}quadruple{small-title,text,btn-green}quadruple{small-title,text,btn-orange}quadruple{small-title,text,btn-orange}}row{double{small-title,text,btn-orange}double{small-title,text,btn-red}}"
          ]
        }
      ],
      "execution_count": 39,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for j, data in enumerate(test_loader):\n",
        "    # (images_test, captions_test, lengths_test)\n",
        "    print(j)\n",
        "    #features = encoder(images_test)\n",
        "    #features"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n"
          ]
        }
      ],
      "execution_count": 47,
      "metadata": {
        "collapsed": false,
        "outputHidden": true,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for images, captions, lengths in test_loader:\n",
        "    #print(images.size())\n",
        "    features = encoder(images)\n",
        "    #print(features.size())\n",
        "    break\n",
        "sampled_ids = []\n",
        "inputs = features.unsqueeze(1)\n",
        "inputs.size()\n",
        "states = None\n",
        "for i in range(77):\n",
        "    hiddens, states = decoder.lstm(inputs, states)\n",
        "    outputs = decoder.fc(hiddens.squeeze(1))\n",
        "    _, predicted = outputs.max(1)\n",
        "    sampled_ids.append(predicted)\n",
        "    inputs = decoder.embed(predicted)\n",
        "    inputs = inputs.unsqueeze(1)\n",
        "sampled_ids = torch.stack(sampled_ids, 1)  \n",
        "sampled_ids.size()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 42,
          "data": {
            "text/plain": [
              "torch.Size([2, 77])"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 42,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = 1\n",
        "print(sampled_ids[n,:])\n",
        "print(captions[n,:])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 12,  13,   1,   7,   0,  14,   0,   7,   2,   6,   1,   5,\n",
            "          1,   3,   0,   4,   0,   8,   2,   5,   1,   3,   0,   4,\n",
            "          0,   8,   2,   5,   1,   3,   0,   4,   0,   8,   2,   5,\n",
            "          1,   3,   0,   4,   0,   8,   2,   2,   6,   1,  11,   1,\n",
            "          3,   0,   4,   0,   8,   2,  11,   1,   3,   0,   4,   0,\n",
            "          8,   2,   2,  15,  15,  15,  12,  13,   1,   7,   0,   7,\n",
            "          0,   7,   0,  14,   0])\n",
            "tensor([ 12,  13,   1,   7,   0,  14,   0,   7,   2,   6,   1,  11,\n",
            "          1,   3,   0,   4,   0,   8,   2,  11,   1,   3,   0,   4,\n",
            "          0,   9,   2,   2,   6,   1,   5,   1,   3,   0,   4,   0,\n",
            "          8,   2,   5,   1,   3,   0,   4,   0,  10,   2,   5,   1,\n",
            "          3,   0,   4,   0,   8,   2,   5,   1,   3,   0,   4,   0,\n",
            "         10,   2,   2,   6,   1,  18,   1,   3,   0,   4,   0,   9,\n",
            "          2,   2,  15])\n"
          ]
        }
      ],
      "execution_count": 44,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## model saved"
      ],
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {}
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.2",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "nteract": {
      "version": "0.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}